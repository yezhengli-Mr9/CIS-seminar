\relax 
\bibstyle{plainnat}
\citation{DBLP:journals/corr/TishbyZ15}
\citation{DBLP:journals/corr/abs-1303-5778,DBLP:journals/corr/ZhangL15,DBLP:journals/corr/abs-1207-0580,DBLP:journals/corr/HeZRS15,natureDeepLeraning}
\citation{probes2016}
\citation{DBLP:journals/corr/TishbyZ15}
\citation{DBLP:journals/corr/Tishby1999}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\newlabel{Introduction}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The DNN layers form a Markov chain of successive internal representations of the input layer $X$. Any representation of the input, $T$, is defined through an encoder, $P(T|X)$, and a decoder $P(\mathaccentV {hat}05E{Y}|T)$, and can be quantified by its \emph  {information plane} coordinates: $I_X= I(X;T)$ and $I_Y=I(T;Y)$. The Information Bottleneck bound characterizes the optimal representations, which maximally compress the input $X$, for a given mutual information on the desired output $Y$. After training, the network receives an input $X$, and successively processes it through the layers, which form a Markov chain, to the predicted output $\mathaccentV {hat}05E{Y}$. $I(Y;\mathaccentV {hat}05E{Y})/I(X;Y)$ quantifies how much of the relevant information is captured by the network. }}{2}}
\newlabel{DNN-layers}{{1}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of Minh's domiant terms, means and variances of the matrices of interest. (NT: not attempted)}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Summary of results and structure of the paper}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Information Theory of Deep Learning}{4}}
\newlabel{Sec:DL-IT}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mutual Information}{4}}
\newlabel{MI}{{2}{4}}
\citation{Cover:2006}
\citation{Cover:2006}
\citation{DBLP:journals/corr/Tishby1999}
\newlabel{eqn:invariance}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Information Plane}{5}}
\newlabel{I-Plane}{{2.2}{5}}
\newlabel{DPI-1}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The Information Bottleneck optimal representations}{5}}
\newlabel{sec:IB}{{2.3}{5}}
\citation{DBLP:journals/corr/Tishby1999}
\citation{MoshkovichTishby17}
\newlabel{eqn:IB}{{9}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}The crucial role of noise}{6}}
\citation{DBLP:journals/corr/TishbyZ15}
\citation{Paninski:2003:EEM:795523.795524}
\citation{PhysRevE.69.066138}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Visualizing DNNs in the Information Plane}{7}}
\citation{Kazhdan2003}
\citation{Kazhdan2003}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical Experiments and Results}{8}}
\newlabel{Sec:Experiments}{{3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental Setup }{8}}
\newlabel{eq:rule}{{10}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Estimating the Mutual Information of the Layers}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The dynamics of the training by Stochastic-Gradient-Decent}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Snapshots of layers (different colors) of 50 randomized networks during the SGD optimization process in the \textit  {information plane} (in bits): \textbf  {left} - with the initial weights; \textbf  {center} - at 400 epochs; \textbf  {right} - after 9000 epochs. The reader is encouraged to view the full videos of this optimization process in the \textit  {information plane} at \textit  {https://goo.gl/rygyIT} and \textit  {https://goo.gl/DQWuDD}.}}{9}}
\newlabel{opt_process}{{2}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The two optimization phases in the Information Plane}{9}}
\citation{Larochelle:2009:EST:1577069.1577070}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The evolution of the layers with the training epochs in the information plane, for different training samples. On the left - 5\% of the data, middle - 45\% of the data, and right - 85\% of the data. The colors indicate the number of training epochs with Stochastic Gradient Descent from 0 to 10000. The network architecture was fully connected layers, with widths: input=12-10-8-6-4-2-1=output. The examples were generated by the spherical symmetric rule described in the text. The green paths correspond to the SGD drift-diffusion phase transition - grey line on Figure 4\hbox {} }}{10}}
\newlabel{network_epochs}{{3}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}The drift and diffusion phases of SGD optimization}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {The layers' Stochastic Gradients distributions during the optimization process.} The norm of the means and standard deviations of the weights gradients for each layer, as function of the number of training epochs (in log-log scale). The values are normalized by the L2 norms of the weights for each layer, which significantly increase during the optimization. The grey line ($\sim 350$ epochs) marks the transition between the first phase, with large gradient means and small variance (\emph  {drift}, high gradient SNR), and the second phase, with large fluctuations and small means (\emph  {diffusion}, low SNR). Note that the gradients log (SNR) (the log differences between the mean and the STD lines) approach a constant for all the layers, reflecting the convergence of the network to a configuration with constant flow of relevant information through the layers! }}{11}}
\newlabel{fig:gradients}{{4}{11}}
\citation{risken1989fokker}
\citation{DBLP:journals/corr/ZhangBHRV16}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}The computational benefit of the hidden layers}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {The layers information paths during the SGD optimization for different architectures.} Each panel is the \textit  {information plane} for a network with a different number of hidden layers. The width of the hidden layers start with 12, and each additional layer has 2 fewer neurons. The final layer with 2 neurons is shown in all panels. The line colors correspond to the number of training epochs. }}{13}}
\newlabel{layers_inf}{{5}{13}}
\citation{Kadmon2016OptimalAI}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}The computational benefits of layered diffusion}{14}}
\newlabel{com.benefit}{{3.7}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Convergence to the layers to the Information Bottleneck bound}{14}}
\newlabel{IB.optimal}{{3.8}{14}}
\citation{cho2015much}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  { The DNN layers converge to fixed-points of the IB equations}. The error bars represent standard error measures with N=50. In each line there are 5 points for the different layers. For each point, $\beta $ is the optimal value that was found for the corresponding layer.}}{15}}
\newlabel{IB}{{6}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Evolution of the layers with training sample size}{15}}
\citation{2016arXiv161101353A}
\citation{Kadmon2016OptimalAI}
\citation{DBLP:journals/corr/BalduzziFLLMM17}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {The effect of the training data size on the layers in the \textit  {information plane}.} Each line (color) represents a converged network with a different training sample size. Along each line there are 6 points for the different layers, each averaged over 50 random training samples and randomized initial weights.}}{16}}
\newlabel{samples_layers}{{7}{16}}
\citation{Haykin:1998:NNC:521706}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{17}}
\newlabel{Sec:discussion}{{4}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {The layers information plane paths (left) and stochastic gradients means and standard deviations for a non-symmetric committee machine rule.} Clearly seen are the two phases of the optimization process as in the symmetric rule. One can also see the equilibration of the gradient SNR for the different layers. While the compression phase is faster in this case, the overall training dynamics is very similar.}}{17}}
\newlabel{committee-machine}{{8}{17}}
\citation{2016arXiv161101353A}
\citation{DBLP:journals/corr/BalduzziFLLMM17}
\citation{geman1988stochastic}
\bibdata{deepIB}
\bibcite{2016arXiv161101353A}{{1}{2016}{{{Achille} and {Soatto}}}{{}}}
\bibcite{probes2016}{{2}{2016}{{Alain and Bengio}}{{}}}
\bibcite{DBLP:journals/corr/BalduzziFLLMM17}{{3}{2017}{{Balduzzi et~al.}}{{Balduzzi, Frean, Leary, Lewis, Ma, and McWilliams}}}
\bibcite{cho2015much}{{4}{2015}{{Cho et~al.}}{{Cho, Lee, Shin, Choy, and Do}}}
\newlabel{app:theorem}{{4}{18}}
\bibcite{Cover:2006}{{5}{2006}{{Cover and Thomas}}{{}}}
\bibcite{geman1988stochastic}{{6}{1988}{{Geman and Geman}}{{}}}
\bibcite{DBLP:journals/corr/abs-1303-5778}{{7}{2013}{{Graves et~al.}}{{Graves, Mohamed, and Hinton}}}
\bibcite{Haykin:1998:NNC:521706}{{8}{1998}{{Haykin}}{{}}}
\bibcite{DBLP:journals/corr/HeZRS15}{{9}{2015}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{DBLP:journals/corr/abs-1207-0580}{{10}{2012}{{Hinton et~al.}}{{Hinton, Srivastava, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{Kadmon2016OptimalAI}{{11}{2016}{{Kadmon and Sompolinsky}}{{}}}
\bibcite{Kazhdan2003}{{12}{2003}{{Kazhdan et~al.}}{{Kazhdan, Funkhouser, and Rusinkiewicz}}}
\bibcite{PhysRevE.69.066138}{{13}{2004}{{Kraskov et~al.}}{{Kraskov, St\"ogbauer, and Grassberger}}}
\bibcite{Larochelle:2009:EST:1577069.1577070}{{14}{2009}{{Larochelle et~al.}}{{Larochelle, Bengio, Louradour, and Lamblin}}}
\bibcite{natureDeepLeraning}{{15}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{MoshkovichTishby17}{{16}{2017}{{Moshkovich and Tishby}}{{}}}
\bibcite{Paninski:2003:EEM:795523.795524}{{17}{2003}{{Paninski}}{{}}}
\bibcite{risken1989fokker}{{18}{1989}{{Risken}}{{}}}
\bibcite{DBLP:journals/corr/TishbyZ15}{{19}{2015}{{Tishby and Zaslavsky}}{{}}}
\bibcite{DBLP:journals/corr/Tishby1999}{{20}{1999}{{Tishby et~al.}}{{Tishby, Pereira, and Bialek}}}
\bibcite{DBLP:journals/corr/ZhangBHRV16}{{21}{2016}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht, and Vinyals}}}
\bibcite{DBLP:journals/corr/ZhangL15}{{22}{2015}{{Zhang and LeCun}}{{}}}
