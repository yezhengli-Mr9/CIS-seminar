
\begin{abstract}
  We study the problem of attributing the prediction of a deep network
  to its input features, a problem previously studied by several other
  works. We identify two fundamental axioms---\emph{Sensitivity} and
  \emph{Implementation Invariance} that attribution methods ought to
  satisfy. We show that they are not satisfied by most known
  attribution methods, which we consider to be a fundamental weakness
  of those methods. We use the axioms to guide the design of a new
  attribution method called \emph{Integrated Gradients}. Our method
  requires no modification to the original network and is extremely
  simple to implement; it just needs a few calls to the standard
  gradient operator. We apply this method to a couple of image models,
  a couple of text models and a chemistry model, demonstrating its
  ability to debug networks, to extract rules from a network, and
  to enable users to engage with models better.
\end{abstract}

\section{Motivation and Summary of Results}\label{sec:intro}

We study the problem of \emph{attributing the prediction of a deep
  network to its input features}.
\begin{definition}
  Formally, suppose we have a function $F: \reals^n \rightarrow [0,1]$
  that represents a deep network, and an input $x = (x_1,\ldots,x_n) \in \reals^n$.
  An attribution of the prediction at input $x$ relative to a baseline
  input $\xbase$ is a vector $A_F(x, \xbase) = (a_1,\ldots,a_n) \in \reals^n$
  where $a_i$ is the \emph{contribution} of $x_i$ to the
  prediction $F(x)$.
\end{definition}
\newpage 
For instance, in an object
recognition network, an attribution method could tell us which pixels
of the image were responsible for a certain label being picked (see
Figure~\ref{fig:intgrad-finalgrad}). The attribution problem was
previously studied by various papers
~\cite{BSHKHM10,SVZ13,SGSK16,BMBMS16,SDBR14}.

The intention of these works is to understand the input-output
behavior of the deep network, which gives us the ability to improve
it.  Such understandability is critical to all computer programs,
including machine learning models. There are also other applications
of attribution. They could be used within a product driven by machine
learning to provide a rationale for the recommendation. For instance,
a deep network that predicts a condition based on imaging could help
inform the doctor of the part of the image that resulted in the
recommendation. This could help the doctor understand the strengths
and weaknesses of a model and compensate for it. We give such an
example in Section~\ref{sec:app-dr}. Attributions could also be used
by developers in an exploratory sense. For instance, we could use a
deep network to extract insights that could be then used in a rule-based
system. In Section~\ref{sec:app-qc}, we give such an example.

A significant challenge in designing an attribution technique is that
they are hard to evaluate empirically. As we discuss in
Section~\ref{sec:uniqueness}, it is hard to tease apart errors that
stem from the misbehavior of the model versus the misbehavior of the
attribution method. To compensate for this shortcoming, we take an
axiomatic approach. In Section~\ref{sec:two-axioms} we identify two
axioms that every attribution method must satisfy. Unfortunately
most previous methods do not satisfy one of these two axioms.
In Section~\ref{sec:method}, we use the axioms to identify a new method,
called \textbf{integrated gradients}.

Unlike previously proposed methods, integrated gradients do not need
any instrumentation of the network, and can be computed easily using
a few calls to the gradient operation, allowing even novice
practitioners to easily apply the technique.

In Section~\ref{sec:app}, we demonstrate the ease of applicability over
several deep networks, including two images networks, two
text processing networks, and a chemistry network. These
applications demonstrate the use of our technique in either improving
our understanding of the network, performing debugging, performing rule
extraction, or aiding an end user in understanding the network's
prediction.

\begin{remark}
  \label{rem:baseline}
  Let us briefly examine the need for the baseline in the definition of
  the attribution problem.
  A common way for humans to perform attribution relies on counterfactual intuition.
  When we assign blame to a certain cause we implicitly consider the absence of
  the cause as a baseline for comparing outcomes. In a deep network, we model
  the absence using a single baseline input. For most deep networks, a natural
  baseline exists in the input space where the prediction is neutral. For instance,
  in object recognition networks, it is the black image. The need for a baseline
  has also been pointed out by prior work on attribution~\cite{SGSK16, BMBMS16}.
\end{remark}

%% \begin{remark}
%%  We do not attempt to detect the kind of logic the network employs,
%%  just the importance of a feature. So in no sense does solving the
%%  attribution problem imply a comprehensive understanding of the
%%  network.
%%  Further, correlations between input features could confound
%%  attributions. If there are two features that frequently co-occur, the
%%  model is free to assign weight to either or both features. The
%%  attributions would then reflect this weight assignment. But, it could
%%  be that the specific weight assignment chosen by the model is not
%%  human-intelligible. These caveats apply to all attribution methods,
%%  not just ours.
%% \end{remark}
